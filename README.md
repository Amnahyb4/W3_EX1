
# ğŸ§  Predictive Modeling with scikit-learn (MOOC Exercises)

This repository contains my solutions and experiments for the **scikit-learn MOOC** on predictive modeling.  
It covers the full machine learning pipeline, from **data exploration** to **model evaluation and hyperparameter tuning**, using real-world datasets.

---

##  Covered Modules & Exercises

### ğŸ”¹ Module M1 â€” Predictive Modeling Pipeline

#### **M1.01 â€“ Tabular Data Exploration**
- Loaded and explored tabular datasets using **pandas**
- Identified **features vs target**
- Distinguished **numerical vs categorical** variables
- Explored class distributions
- Visualized feature distributions using:
  - Histograms
  - Pair plots (seaborn)

ğŸ“‚ Dataset:
- `penguins_classification.csv`

---

#### **M1.02 â€“ First Model on Numerical Data**
- Built a first classifier using **KNeighborsClassifier**
- Learned how to use:
  - `fit()`
  - `predict()`
  - `score()`
- Investigated default hyperparameters (`n_neighbors`)
- Performed a **train-test split**

ğŸ“‚ Dataset:
- `adult-census-numeric.csv`

---

#### **M1.03 â€“ Baseline Models & Model Evaluation**
- Introduced **baseline models** using `DummyClassifier`
- Compared model performance against:
  - Always predicting the majority class
- Discussed whether ~81â€“82% accuracy is meaningful
- Evaluated generalization using **train/test split**

ğŸ“‚ Dataset:
- `adult-census.csv`

---

#### **M1.04 â€“ Handling Categorical Data**
- Worked with **categorical variables**
- Used:
  - `OrdinalEncoder`
  - `OneHotEncoder`
- Built **pipelines** with `LogisticRegression`
- Evaluated models using **cross-validation**
- Observed that:
  - One-hot encoding performs better for linear models
  - Ordinal encoding can introduce artificial ordering

> **Key insight:**  
> Categorical features do **not** have an inherent order â†’ `OneHotEncoder` is usually more appropriate for linear models.

---

#### **M1.05 â€“ Numerical + Categorical Features Together**
- Used **ColumnTransformer** to:
  - Scale numerical features
  - Encode categorical features
- Compared preprocessing strategies for:
  - Linear models
  - Tree-based models
- Learned that:
  - Tree-based models do **not** require feature scaling
  - Ordinal encoding is acceptable for trees

Models used:
- `LogisticRegression`
- `HistGradientBoostingClassifier`

---

### ğŸ”¹ Module M2 â€” Model Validation

#### **Cross-Validation**
- Used `cross_validate`
- Understood the difference between:
  - Training score
  - Test score
- Learned that cross-validation is used for:
  - Estimating **generalization performance**
  - Measuring **performance uncertainty**
- Analyzed:
  - Mean accuracy
  - Standard deviation
  - Fit time vs score time

ğŸ“‚ Dataset:
- `blood_transfusion.csv`

---

### ğŸ”¹ Module M3 â€” Hyperparameter Tuning

#### **M3.01 â€“ Manual Hyperparameter Tuning**
- Performed **manual search** over hyperparameters
- Tuned parameters for:
  - Tree-based models
- Used:
  - Smaller training subsets for faster iteration
  - `train_size` to control computation cost

Model:
- `HistGradientBoostingClassifier`

---

## Technologies Used

- Python 3.11
- pandas
- numpy
- matplotlib
- seaborn
- scikit-learn
- Jupyter Notebooks

---

## ğŸ“ Repository Structure

```text
.
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ adult-census.csv
â”‚   â”œâ”€â”€ adult-census-numeric.csv
â”‚   â”œâ”€â”€ blood_transfusion.csv
â”‚   â””â”€â”€ penguins_classification.csv
â”‚
â”œâ”€â”€ 01_tabular_data_exploration_ex_01.ipynb
â”œâ”€â”€ 02_numerical_pipeline_ex_01.ipynb
â”œâ”€â”€ 03_categorical_pipeline_ex_01.ipynb
â”œâ”€â”€ 03_categorical_pipeline_ex_02.ipynb
â”œâ”€â”€ cross_validation_ex_01.ipynb
â”œâ”€â”€ parameter_tuning_ex_01.ipynb
â”œâ”€â”€ parameter_tuning_ex_02.ipynb
â””â”€â”€ README.md

##  Learning Outcomes

- Built end-to-end machine learning pipelines using scikit-learn  
- Explored and prepared tabular data with numerical and categorical features  
- Applied appropriate encoding strategies and preprocessing techniques  
- Evaluated models using cross-validation and baseline comparisons  
- Performed hyperparameter tuning and analyzed model performance  
