{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù Exercise M1.04\n",
    "\n",
    "The goal of this exercise is to evaluate the impact of using an arbitrary\n",
    "integer encoding for categorical variables along with a linear classification\n",
    "model such as Logistic Regression.\n",
    "\n",
    "To do so, let's try to use `OrdinalEncoder` to preprocess the categorical\n",
    "variables. This preprocessor is assembled in a pipeline with\n",
    "`LogisticRegression`. The generalization performance of the pipeline can be\n",
    "evaluated by cross-validation and then compared to the score obtained when\n",
    "using `OneHotEncoder` or to some other baseline score.\n",
    "\n",
    "First, we load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "adult_census = pd.read_csv(\"dataset/adult-census.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = \"class\"\n",
    "target = adult_census[target_name]\n",
    "data = adult_census.drop(columns=[target_name, \"education-num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, we used `sklearn.compose.make_column_selector` to\n",
    "automatically select columns with a specific data type (also called `dtype`).\n",
    "Here, we use this selector to get only the columns containing strings (column\n",
    "with `object` dtype) that correspond to categorical features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    workclass      education       marital-status          occupation  \\\n",
      "0     Private           11th        Never-married   Machine-op-inspct   \n",
      "1     Private        HS-grad   Married-civ-spouse     Farming-fishing   \n",
      "2   Local-gov     Assoc-acdm   Married-civ-spouse     Protective-serv   \n",
      "3     Private   Some-college   Married-civ-spouse   Machine-op-inspct   \n",
      "4           ?   Some-college        Never-married                   ?   \n",
      "\n",
      "  relationship    race      sex  native-country  \n",
      "0    Own-child   Black     Male   United-States  \n",
      "1      Husband   White     Male   United-States  \n",
      "2      Husband   White     Male   United-States  \n",
      "3      Husband   Black     Male   United-States  \n",
      "4    Own-child   White   Female   United-States  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import make_column_selector as selector\n",
    "\n",
    "categorical_columns_selector = selector(dtype_include=object)\n",
    "categorical_columns = categorical_columns_selector(data)\n",
    "data_categorical = data[categorical_columns]\n",
    "print(data_categorical.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a scikit-learn pipeline composed of an `OrdinalEncoder` and a\n",
    "`LogisticRegression` classifier.\n",
    "\n",
    "Because `OrdinalEncoder` can raise errors if it sees an unknown category at\n",
    "prediction time, you can set the `handle_unknown=\"use_encoded_value\"` and\n",
    "`unknown_value` parameters. You can refer to the [scikit-learn\n",
    "documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)\n",
    "for more details regarding these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model= make_pipeline(\n",
    "    OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "065f5f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.pipeline.Pipeline'>\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "print(hasattr(model, \"fit\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model is now defined. Evaluate it using a cross-validation using\n",
    "`sklearn.model_selection.cross_validate`.\n",
    "\n",
    "<div class=\"admonition note alert alert-info\">\n",
    "<p class=\"first admonition-title\" style=\"font-weight: bold;\">Note</p>\n",
    "<p class=\"last\">Be aware that if an error happened during the cross-validation,\n",
    "<tt class=\"docutils literal\">cross_validate</tt> would raise a warning and return NaN (Not a Number) as scores.\n",
    "To make it raise a standard Python exception with a traceback, you can pass\n",
    "the <tt class=\"docutils literal\"><span class=\"pre\">error_score=\"raise\"</span></tt> argument in the call to <tt class=\"docutils literal\">cross_validate</tt>. An\n",
    "exception would be raised instead of a warning at the first encountered problem\n",
    "and <tt class=\"docutils literal\">cross_validate</tt> would stop right away instead of returning NaN values.\n",
    "This is particularly handy when developing complex machine learning pipelines.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fit_time  score_time  test_score  train_score\n",
      "count  5.000000    5.000000    5.000000     5.000000\n",
      "mean   0.335544    0.023603    0.755477     0.755487\n",
      "std    0.054917    0.007289    0.001715     0.000639\n",
      "min    0.286339    0.016664    0.753071     0.754722\n",
      "25%    0.301442    0.019783    0.755144     0.754920\n",
      "50%    0.322890    0.022826    0.755553     0.755662\n",
      "75%    0.340425    0.022943    0.755733     0.755950\n",
      "max    0.426624    0.035799    0.757883     0.756181\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    model, data_categorical, target, cv=5, return_train_score=True, error_score='raise'\n",
    ")\n",
    "\n",
    "cv_results_df = pd.DataFrame(cv_results)\n",
    "print(cv_results_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to compare the generalization performance of our previous\n",
    "model with a new model where instead of using an `OrdinalEncoder`, we use a\n",
    "`OneHotEncoder`. Repeat the model evaluation using cross-validation. Compare\n",
    "the score of both models and conclude on the impact of choosing a specific\n",
    "encoding strategy when using a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "model2= make_pipeline(\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ad9afd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fit_time  score_time  test_score  train_score\n",
      "count  5.000000    5.000000    5.000000     5.000000\n",
      "mean   0.182960    0.017830    0.832849     0.833975\n",
      "std    0.017900    0.002754    0.002893     0.000672\n",
      "min    0.169916    0.014215    0.828317     0.833521\n",
      "25%    0.170455    0.015849    0.832327     0.833568\n",
      "50%    0.178491    0.018523    0.832924     0.833598\n",
      "75%    0.182406    0.019629    0.834971     0.834080\n",
      "max    0.213536    0.020933    0.835705     0.835108\n"
     ]
    }
   ],
   "source": [
    "cross_validate2 = cross_validate(\n",
    "    model2, data_categorical, target, cv=5, return_train_score=True, error_score='raise'\n",
    ")   \n",
    "cv_results2_df = pd.DataFrame(cross_validate2)\n",
    "print(cv_results2_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33445a0f",
   "metadata": {},
   "source": [
    "Compare the result of  both models and conclude on the impact of choosing a specific\n",
    "encoding strategy when using a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240144dc",
   "metadata": {},
   "source": [
    "Accuracy: the accuracy of one-hot encoder is higher than ordinal encoder and that because the categorical feature don't need ordinal encoder which may mislead the logistic regression model \n",
    "\n",
    "speed:one hot enocder is fastest since the fit time mean is 0.18\n",
    "\n",
    "overfitting:both models seems to have no overfitting since the traing and testing score are similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "695130b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\amnah\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\python.exe'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
